# -*- coding: utf-8 -*-
"""Video_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PDFrmTQSvQcA9DBkUTWPg_D6-CR3FnmJ
"""

import cv2
import numpy as np
import tensorflow as tf

model = tf.keras.models.load_model("/content/cnn1_best_model.h5")

emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise','neutral']

face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

video_path = "/content/5.mp4"
cap = cv2.VideoCapture(video_path)

fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

out = cv2.VideoWriter(
    "/content/emotion_prediction.mp4",
    cv2.VideoWriter_fourcc(*"mp4v"),
    fps,
    (width, height)
)

while cap.isOpened():
  ret, frame = cap.read()
  if not ret:
    break

  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

  faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

  for (x,y,w,h) in faces:
    face = frame[y:y+h, x:x+w]

    face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
    face_resized = cv2.resize(face_rgb, (96,96))
    face_array = np.expand_dims(face_resized, axis=0) / 255.0

    prediction = model.predict(face_array, verbose = 0)
    label_idx = np.argmax(prediction)
    label_text = f"{emotion_labels[label_idx]} ({prediction[0][label_idx]*100:.1f}%)"

    # Draw bounding box
    cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)
    cv2.putText(frame, label_text, (x,y-10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)


  out.write(frame)

cap.release()
out.release()

# To download in Colab
from google.colab import files
files.download("/content/emotion_prediction.mp4")

