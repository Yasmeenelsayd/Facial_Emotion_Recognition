# -*- coding: utf-8 -*-
"""Video_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PDFrmTQSvQcA9DBkUTWPg_D6-CR3FnmJ
"""

import cv2
import numpy as np
import tensorflow as tf

model = tf.keras.models.load_model("/content/cnn1_best_model.h5")

emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise','neutral']

face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

cap = cv2.VideoCapture(0)

while cap.isOpened():
  ret, frame = cap.read()
  if not ret:
    break

  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

  faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)

  for (x,y,w,h) in faces:
    face = frame[y:y+h, x:x+w]

    face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
    face_resized = cv2.resize(face_rgb, (96,96))
    face_array = np.expand_dims(face_resized, axis=0) / 255.0

    prediction = model.predict(face_array, verbose = 0)
    label_idx = np.argmax(prediction)
    label_text = f"{emotion_labels[label_idx]} ({prediction[0][label_idx]*100:.1f}%)"

    # Draw bounding box
    cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)
    cv2.putText(frame, label_text, (x,y-10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,0), 2)

    cv2.imshow("Live Facial Emotion Recognition", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

